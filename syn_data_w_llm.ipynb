{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b33e28",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c227a36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T07:16:14.428970Z",
     "start_time": "2024-09-25T07:16:04.915675Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note : Install the dependencies from requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fdbe90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T07:22:19.199993Z",
     "start_time": "2024-09-25T07:22:16.418343Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44e08132",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T05:10:31.522893Z",
     "start_time": "2024-09-29T05:10:31.511030Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Libraries for openai\n",
    "import pandas as pd\n",
    "import re\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from pydantic import BaseModel\n",
    "from langchain_experimental.tabular_synthetic_data.openai import (\n",
    "    OPENAI_TEMPLATE,\n",
    "    create_openai_data_generator,\n",
    ")\n",
    "from langchain_experimental.tabular_synthetic_data.prompts import (\n",
    "    SYNTHETIC_FEW_SHOT_PREFIX,\n",
    "    SYNTHETIC_FEW_SHOT_SUFFIX,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "#Libraries for llama\n",
    "import pandas as pd\n",
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser,RetryOutputParser\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e769fd64",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "24b9fc6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T10:51:37.477620Z",
     "start_time": "2024-09-29T10:51:37.464225Z"
    }
   },
   "outputs": [],
   "source": [
    "WIKI_SAMPLE_FILE = '../data/wikihowAll_sample.parquet'\n",
    "WIKI_SAMPLE_CLEANED_FILE = '../data/wikihowAll_sample_cleaned.parquet'\n",
    "\n",
    "WIKI_QUERY_RAW_FILE = '../data/wikihow_queries_generated_raw_uncleaned.parquet'\n",
    "WIKI_QUERY_GENERATED_FILE = '../data/wikihow_queries_generated.parquet'\n",
    "GEMMA2_QUERY_FILE = \"../data/wikihow_gemma2_queries.parquet\"\n",
    "LLAMA3_QUERY_FILE = \"../data/wikihow_llama3_queries.parquet\"\n",
    "WIKI_QUERY_EVALUATED_FILE = '../data/wikihow_queries_evaluated.parquet'\n",
    "\n",
    "WIKI_QUERY_ANSWER_RAW_FILE = '../data/wikihow_queries_answers_generated_raw_uncleaned.parquet'\n",
    "WIKI_QUERY_ANSWER_GENERATED_FILE = '../data/wikihow_queries_answers_generated.parquet'\n",
    "GEMMA2_QA_DATA_FILE = \"../data/wikihow_gemma2_qa_data.parquet\"\n",
    "LLAMA3_QA_DATA_FILE = \"../data/wikihow_llama3_qa_data.parquet\"\n",
    "GEMMA2_QA_DATA_EVALUATED_FILE = \"../data/wikihow_gemma2_qa_data_evaluated.parquet\"\n",
    "LLAMA3_QA_DATA_EVALUATED_FILE = \"../data/wikihow_llama3_qa_data_evaluated.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db80317e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T17:00:53.292356Z",
     "start_time": "2024-09-27T17:00:46.744092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/mahnazkoupaee/WikiHow-Dataset?tab=readme-ov-file\n",
    "## Wikihowall.csv link is below\n",
    "#https://ucsb.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358\n",
    "df_all = pd.read_csv('wikihowAll.csv')\n",
    "## dataset is very big we take specific samples\n",
    "df_all = df_all.sample(frac=0.001)\n",
    "df_all.to_parquet(WIKI_SAMPLE_FILE)\n",
    "\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c49c0",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "95e04cbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T13:15:43.457774Z",
     "start_time": "2024-09-28T13:15:43.351503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 2)\n",
      "(110, 2)\n"
     ]
    }
   ],
   "source": [
    "# Convert all columns to string type\n",
    "#https://github.com/mahnazkoupaee/WikiHow-Dataset/blob/master/process.py\n",
    "import re\n",
    "\n",
    "df_all = pd.read_parquet(WIKI_SAMPLE_FILE)\n",
    "df_all = df_all.astype(str)\n",
    "\n",
    "# Get the shape of the dataframe\n",
    "rows, columns =df_all.shape\n",
    "\n",
    "# Define cleaning function\n",
    "def clean_data(row):\n",
    "    abstract = row['headline']\n",
    "    article = row['text']\n",
    "    \n",
    "    # Common cleaning function for both abstract and article\n",
    "    def clean_text(text):\n",
    "        # Remove extra commas and periods followed by commas\n",
    "        text = text.replace(\".,\", \".\")\n",
    "        \n",
    "        # Remove extra commas and newline characters\n",
    "        text = re.sub(r'[.]+[\\n]+[,]', \". \", text)\n",
    "        \n",
    "        # Remove newline characters\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        \n",
    "        # Strip leading and trailing punctuation and special characters from words\n",
    "        text = re.sub(r'^[^\\w\\s]+|[^\\w\\s]+$', '', text)  # Remove leading/trailing special characters\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    # Apply cleaning to both abstract and article\n",
    "    abstract = clean_text(abstract)\n",
    "    article = clean_text(article)\n",
    "    \n",
    "    # Check if the cleaned text contains meaningful content\n",
    "    if not re.search(r'\\w', abstract) or not re.search(r'\\w', article):\n",
    "        return None  # Drop this row if no meaningful content is found\n",
    "    \n",
    "    # Drop rows if the abstract or article is too short (e.g., less than 100 characters)\n",
    "    if len(abstract) < 80 or len(article) < 1400:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        'headline': abstract,\n",
    "        'text': article\n",
    "    }\n",
    "    \n",
    "\n",
    "# Apply cleaning function to each row\n",
    "df = df_all.apply(clean_data, axis=1)\n",
    "\n",
    "# Remove None values (rows that didn't meet the threshold)\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert the cleaned data back to a dataframe\n",
    "df = df.apply(pd.Series)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.shape)\n",
    "if df.shape[0] > 110:\n",
    "    df = df.head(110)\n",
    "    \n",
    "print(df.shape)\n",
    "#df.to_parquet(WIKI_SAMPLE_CLEANED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "627877e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T13:15:44.961373Z",
     "start_time": "2024-09-28T13:15:44.939600Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(WIKI_SAMPLE_CLEANED_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb57e957",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d84309",
   "metadata": {},
   "source": [
    "## Option 1: LLM :: To Generate both q & a from context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bffd918",
   "metadata": {},
   "source": [
    "### Prompt for Generating Synthetic data q & a from context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d9b2a71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T05:49:25.825920Z",
     "start_time": "2024-09-29T05:49:25.812551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define prompt template for generating questions\n",
    "\n",
    "QUERY_AND_ANSWER_GENERATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI assistant designed to generate realistic user queries and its corresponding answer related to various topics.\n",
    "Your task is to create diverse, natural-sounding questions that reflect what actual users might ask when seeking\n",
    "information on a specific subject.\n",
    "\n",
    "**Key aspects:**\n",
    "Generate 5 diverse, natural, short, specific, and concise user queries (â‰¤ 10 words) and its corresponding answers (60-150 words)\n",
    "based solely on the provided topic.\n",
    "Do not refer to external sources or use prior knowledge.\n",
    "Only use the provided topic to generate queries and answers. \n",
    "Ensure all queries have answers and ensure the answer generated is a direct and relevant response to the query.\n",
    "Mimic real users seeking information on a chatbot platform.\n",
    "Use simple language and maintain a neutral, conversational tone.\n",
    "\n",
    "**Guidelines:**\n",
    "1. Vary complexity, specificity, and format.\n",
    "2. Mimic chatbot users seeking information.\n",
    "3. Use simple language and maintain a neutral tone.\n",
    "4. Avoid ambiguity and ensure specificity.\n",
    "5. Ensure all queries have answers.\n",
    "6. Ensure an answer directly and specifically addresses its query instead of giving vaugue answer.\n",
    "7. Answers must directly address the query, providing specific and relevant information directly instead of referencing external sources.\n",
    "8. If a query asks for steps, provide clear, numbered steps in the answer.\n",
    "\n",
    "**Additional Requirements:**\n",
    "Ensure query-answer pairs are logically connected and relevant to the topic.\n",
    "When queries request procedures or steps, answers must provide sequential, numbered  instructions.\n",
    "When queries ask for size or shape, answers must: Specify exact measurements (where applicable), Describe proportions or dimensions\n",
    "Use concise and clear language and avoid vague or open-ended answers..\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "Query relevance to the topic.\n",
    "Answer accuracy and relevance to the query.\n",
    "Answer completeness (60-150 words).\n",
    "Query-answer alignment: How well does the answer respond to the query?\n",
    "Step-by-step clarity (when applicable)\n",
    "Size/shape explanation clarity (when applicable)\n",
    "\n",
    "Example Input:\n",
    "Topic: ### Planning a Budget-Friendly Vacation \n",
    "Planning a budget-friendly vacation requires careful research and flexibility. When planning a vacation on a budget,\n",
    "consider destinations with affordable accommodations, transportation, and activities, such as:\n",
    "Thailand: Known for its affordable beaches, street food, and cultural attractions.\n",
    "Vietnam: Offers affordable accommodations, delicious cuisine, and scenic landscapes.\n",
    "Peru: Provides affordable Inca Trail hikes, cultural experiences, and historical sites.\n",
    "Researching and booking in advance can also help save money. Other budget-friendly strategies include:\n",
    "Booking budget-friendly accommodations like hostels or guesthouses.\n",
    "Using public transportation or budget airlines.\n",
    "Planning free or low-cost activities like hiking or exploring local markets.\n",
    "\n",
    "Example Queries and Answers:\n",
    "Query 1: What are the best budget-friendly destinations worldwide?\n",
    "Answer 1: Consider Thailand, Vietnam, or Peru for affordable accommodations and activities.\n",
    "Query 2: How can I save money on flights and accommodations?\n",
    "Answer 2: Book in advance, use travel rewards credit cards, and explore budget airlines.\n",
    "Query 3: What's the cheapest way to travel across Europe?\n",
    "Answer 3: Use budget airlines, buses, or trains, and book accommodations through hostels.\n",
    "Query 4: How do I plan a 5-day budget trip from Singapore to Hawaii?\n",
    "Answer 4: Research affordable flights, book budget-friendly accommodations, and plan free activities.\n",
    "Query 5: What budgeting apps are best for travel expenses?\n",
    "Answer 5: Popular options include Mint, Trail Wallet, and Budget Your Trip.\n",
    "\n",
    "Generate the output in the list of string format\n",
    "{format_instructions}\n",
    "\n",
    "Now, generate user queries based on the following topic:\n",
    "{topic}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93b7c5",
   "metadata": {},
   "source": [
    "###  Generating Synthetic data q & a from context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f02fc517",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T05:49:12.501229Z",
     "start_time": "2024-09-29T05:49:12.489579Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def set_generate_model_prompt_chain_for_qa(GENRATIVE_MODEL):\n",
    "    \n",
    "    class LLMGenerate_OutputFormat(BaseModel):\n",
    "        \"\"\"LLM output format\"\"\"\n",
    "        queries: List[str] = Field(description=\"List of string queries generated by the LLM\", min_items=1)\n",
    "        answers: List[str] = Field(description=\"List of string answers generated by the LLM\", min_items=1)\n",
    "        \n",
    "    model_generate_synthetic = Ollama(model=GENRATIVE_MODEL)\n",
    "    \n",
    "    output_parser = PydanticOutputParser(pydantic_object=LLMGenerate_OutputFormat, temperature=0.2)\n",
    "    prompt = PromptTemplate(\n",
    "        template=QUERY_AND_ANSWER_GENERATION_PROMPT_TEMPLATE,\n",
    "        input_variables=[\"topic\"],\n",
    "        partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    "    )\n",
    "    \n",
    "    chain = prompt | model_generate_synthetic \n",
    "    \n",
    "    return chain \n",
    "\n",
    "\n",
    "\n",
    "def llm_generate_queries_and_answers_for_context(topic, chain, GENRATIVE_MODEL):\n",
    "    \"\"\"\n",
    "    Generate queries and answers for a given context topic using LLaMA.\n",
    "\n",
    "    Args:\n",
    "    topic (str): Context topic.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], List[str]]: List of generated queries and answers.\n",
    "\n",
    "    Raises:\n",
    "    Exception: If LLaMA invocation fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output = chain.invoke({\"topic\": topic})\n",
    "        output = output.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        \n",
    "        print(\"After cleaning ***\")\n",
    "        print(type(output))\n",
    "        print(output)\n",
    "        \n",
    "        if(GENRATIVE_MODEL==\"gemma2:2b\"):\n",
    "            parsed_data = json.loads(output)\n",
    "\n",
    "            # Extract queries and answers\n",
    "            queries = parsed_data['queries']\n",
    "            answers = parsed_data['answers']\n",
    "            return queries, answers\n",
    "        \n",
    "        elif(GENRATIVE_MODEL==\"llama3\"):\n",
    "            llm_output_str = output.strip()\n",
    "\n",
    "            # Remove leading/trailing whitespace and newline characters\n",
    "            llm_output_str = llm_output_str.replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "\n",
    "            # Find the JSON part\n",
    "            json_start = llm_output_str.find(\"{\")\n",
    "            json_end = llm_output_str.rfind(\"}\")\n",
    "\n",
    "            # Extract JSON\n",
    "            json_str = llm_output_str[json_start:json_end+1]\n",
    "            print(json_str)\n",
    "\n",
    "            # Check if JSON string is valid\n",
    "            if json_str and json_str.startswith(\"{\") and json_str.endswith(\"}\"):\n",
    "                try:\n",
    "                    # Parse JSON\n",
    "                    llm_output = json.loads(json_str)\n",
    "\n",
    "                    # Extract queries and answers\n",
    "                    queries = llm_output.get(\"queries\", [])\n",
    "                    answers = llm_output.get(\"answers\", [])\n",
    "                    \n",
    "                    return queries, answers\n",
    "                except json.JSONDecodeError as e:\n",
    "                    # Handle JSON parsing errors\n",
    "                    print(f\"JSON parsing error: {e}\")\n",
    "                    return [], []\n",
    "            else:\n",
    "                # Handle invalid JSON format\n",
    "                print(\"Invalid JSON format\")\n",
    "                return [], []\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                data_dict = json.loads(output)\n",
    "\n",
    "                # Extract queries and answers\n",
    "                queries = data_dict['queries']\n",
    "                answers = data_dict['answers']\n",
    "                \n",
    "                return queries, answers\n",
    "            except (json.JSONDecodeError, KeyError) as e:\n",
    "                print(f\"Error parsing output: {e}\")\n",
    "                return [], []\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return [], []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a2b15",
   "metadata": {},
   "source": [
    "### Applying Synthetic data q & a Generation from context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8551430c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T15:20:58.082820Z",
     "start_time": "2024-09-28T15:18:22.433618Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning ***\n",
      "<class 'str'>\n",
      "Here are the user queries based on the topic \"Receding Gums\":\n",
      "\n",
      "\n",
      "{\n",
      "  \"queries\": [\n",
      "    \"What causes receding gums?\",\n",
      "    \"How can I prevent gum recession?\",\n",
      "    \"Is smoking a risk factor for receding gums?\",\n",
      "    \"What is the best way to brush my teeth to prevent receding gums?\",\n",
      "    \"Can I fix receding gums on my own or do I need professional help?\"\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    \"Your dentist can help you identify any risk factors that may contribute to receding gums. Typical causes include gum disease, using a hard-bristled toothbrush, brushing too hard, being born with naturally thin or weak gums, smoking and using tobacco, and trauma to your gum tissue.\",\n",
      "    \"Use a soft-bristled toothbrush to gently brush your teeth twice a day, and avoid brushing too hard. Be sure to brush all the different surfaces of your teeth, including the front, back, and top.\",\n",
      "    \"Yes, smoking is a risk factor for receding gums. It can cause inflammation and damage to the gum tissue, leading to recession.\",\n",
      "    \"To prevent receding gums, use a soft-bristled toothbrush to gently brush your teeth twice a day. Hold your toothbrush at a 45-degree angle to your gums, and brush back and forth using short strokes with no pressure on the toothbrush.\",\n",
      "    \"In most cases, you will need professional help from a dentist or specialist to fix receding gums. They can perform scaling and root planning or a gum graft to increase gum growth.\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "\n",
      "Note: The answers provided are direct responses to the queries, providing specific and relevant information related to the topic of receding gums.\n",
      "{  \"queries\": [    \"What causes receding gums?\",    \"How can I prevent gum recession?\",    \"Is smoking a risk factor for receding gums?\",    \"What is the best way to brush my teeth to prevent receding gums?\",    \"Can I fix receding gums on my own or do I need professional help?\"  ],  \"answers\": [    \"Your dentist can help you identify any risk factors that may contribute to receding gums. Typical causes include gum disease, using a hard-bristled toothbrush, brushing too hard, being born with naturally thin or weak gums, smoking and using tobacco, and trauma to your gum tissue.\",    \"Use a soft-bristled toothbrush to gently brush your teeth twice a day, and avoid brushing too hard. Be sure to brush all the different surfaces of your teeth, including the front, back, and top.\",    \"Yes, smoking is a risk factor for receding gums. It can cause inflammation and damage to the gum tissue, leading to recession.\",    \"To prevent receding gums, use a soft-bristled toothbrush to gently brush your teeth twice a day. Hold your toothbrush at a 45-degree angle to your gums, and brush back and forth using short strokes with no pressure on the toothbrush.\",    \"In most cases, you will need professional help from a dentist or specialist to fix receding gums. They can perform scaling and root planning or a gum graft to increase gum growth.\"  ]}\n",
      "\n",
      "Output Queries all:\n",
      "['What causes receding gums?', 'How can I prevent gum recession?', 'Is smoking a risk factor for receding gums?', 'What is the best way to brush my teeth to prevent receding gums?', 'Can I fix receding gums on my own or do I need professional help?']\n",
      "['Your dentist can help you identify any risk factors that may contribute to receding gums. Typical causes include gum disease, using a hard-bristled toothbrush, brushing too hard, being born with naturally thin or weak gums, smoking and using tobacco, and trauma to your gum tissue.', 'Use a soft-bristled toothbrush to gently brush your teeth twice a day, and avoid brushing too hard. Be sure to brush all the different surfaces of your teeth, including the front, back, and top.', 'Yes, smoking is a risk factor for receding gums. It can cause inflammation and damage to the gum tissue, leading to recession.', 'To prevent receding gums, use a soft-bristled toothbrush to gently brush your teeth twice a day. Hold your toothbrush at a 45-degree angle to your gums, and brush back and forth using short strokes with no pressure on the toothbrush.', 'In most cases, you will need professional help from a dentist or specialist to fix receding gums. They can perform scaling and root planning or a gum graft to increase gum growth.']\n",
      "\n",
      "Query 1: What causes receding gums?\n",
      "Your dentist can help you identify any risk factors that may contribute to receding gums. Typical causes include gum disease, using a hard-bristled toothbrush, brushing too hard, being born with naturally thin or weak gums, smoking and using tobacco, and trauma to your gum tissue.\n",
      "Query 2: How can I prevent gum recession?\n",
      "Use a soft-bristled toothbrush to gently brush your teeth twice a day, and avoid brushing too hard. Be sure to brush all the different surfaces of your teeth, including the front, back, and top.\n",
      "Query 3: Is smoking a risk factor for receding gums?\n",
      "Yes, smoking is a risk factor for receding gums. It can cause inflammation and damage to the gum tissue, leading to recession.\n",
      "Query 4: What is the best way to brush my teeth to prevent receding gums?\n",
      "To prevent receding gums, use a soft-bristled toothbrush to gently brush your teeth twice a day. Hold your toothbrush at a 45-degree angle to your gums, and brush back and forth using short strokes with no pressure on the toothbrush.\n",
      "Query 5: Can I fix receding gums on my own or do I need professional help?\n",
      "In most cases, you will need professional help from a dentist or specialist to fix receding gums. They can perform scaling and root planning or a gum graft to increase gum growth.\n"
     ]
    }
   ],
   "source": [
    "GENRATIVE_MODEL = \"llama3\"\n",
    "generation_chain = set_generate_model_prompt_chain_for_qa(GENRATIVE_MODEL)\n",
    "\n",
    "row_num = 2\n",
    "input_text = df.iloc[row_num]['text']\n",
    "queries, answers = llm_generate_queries_and_answers_for_context(input_text,generation_chain,GENRATIVE_MODEL)\n",
    "\n",
    "# Print generated queries\n",
    "print(\"\\nOutput Queries all:\")\n",
    "print(queries)\n",
    "print(answers)\n",
    "print(\"\")\n",
    "for i, query in enumerate(queries, start=1):\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(answers[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b706484d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T05:49:30.315329Z",
     "start_time": "2024-09-29T05:49:30.297290Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GENRATIVE_MODEL = \"gemma2:2b\"\n",
    "generation_chain = set_generate_model_prompt_chain_for_qa(GENRATIVE_MODEL)\n",
    "\n",
    "def llm_generate_queries_answers_for_context_helper(row):\n",
    "    \n",
    "    index = row.name  # Get the index of the current row\n",
    "    print(f\"Processing gemma2 row {index}...\")\n",
    "    queries, answers = llm_generate_queries_and_answers_for_context(row['text'],generation_chain,GENRATIVE_MODEL)  # Generate queries based on the text\n",
    "    return pd.Series([queries, answers])  \n",
    "\n",
    "# Apply the function to each row in the DataFrame and create a new column 'queries'\n",
    "df[['gemma2_queries', 'gemma2_answers']] = df.apply(lambda row: llm_generate_queries_answers_for_context_helper(row), axis=1)\n",
    "df['gemma2_answers'] = df['gemma2_answers'].apply(lambda x: json.dumps(x))\n",
    "df['doc_id'] = df.index\n",
    "df.to_parquet(WIKI_QUERY_ANSWER_RAW_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03fffd07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T05:49:42.341519Z",
     "start_time": "2024-09-29T05:49:42.323233Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GENRATIVE_MODEL = \"llama3\"\n",
    "generation_chain = set_generate_model_prompt_chain_for_qa(GENRATIVE_MODEL)\n",
    "\n",
    "def llm_generate_queries_answers_for_context_helper(row):\n",
    "    index = row.name  # Get the index of the current row\n",
    "    print(f\"Processing row {index}...\")\n",
    "    \n",
    "    queries, answers = llm_generate_queries_and_answers_for_context(row['text'],generation_chain,GENRATIVE_MODEL)  # Generate queries based on the text\n",
    "    return pd.Series([queries, answers])    \n",
    "\n",
    "df=pd.read_parquet(WIKI_QUERY_ANSWER_RAW_FILE)\n",
    "\n",
    "# Apply the function to each row in the DataFrame and create a new column 'queries'\n",
    "df[['llama3_queries', 'llama3_answers']]  = df.apply(lambda row: llm_generate_queries_answers_for_context_helper(row), axis=1)\n",
    "df['llama3_answers'] = df['llama3_answers'].apply(lambda x: json.dumps(x))\n",
    "df.to_parquet(WIKI_QUERY_ANSWER_RAW_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc7ac3",
   "metadata": {},
   "source": [
    "### Clean the Synthetic data generated q & a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3587018",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T05:30:25.447742Z",
     "start_time": "2024-09-29T05:30:25.260315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>gemma2_queries</th>\n",
       "      <th>gemma2_answers</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>llama3_queries</th>\n",
       "      <th>llama3_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Choose your container. Understand why steriliz...</td>\n",
       "      <td>The ideal container is wide, flat and shallow,...</td>\n",
       "      <td>[What type of containers are ideal for collect...</td>\n",
       "      <td>[Containers like baking trays, takeaway food c...</td>\n",
       "      <td>0</td>\n",
       "      <td>[What's the best way to sterilize a container ...</td>\n",
       "      <td>[You can sterilize a container at home using s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avoid bright colors. Choose monochromatic elem...</td>\n",
       "      <td>The ideal palette for a minimalist tablescape ...</td>\n",
       "      <td>[What colors are ideal for a minimalist table ...</td>\n",
       "      <td>[Black and white are ideal, with wood providin...</td>\n",
       "      <td>1</td>\n",
       "      <td>[What colors make a good minimalist palette?, ...</td>\n",
       "      <td>[Select black, white, and wood colors for a cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  Choose your container. Understand why steriliz...   \n",
       "1  Avoid bright colors. Choose monochromatic elem...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The ideal container is wide, flat and shallow,...   \n",
       "1  The ideal palette for a minimalist tablescape ...   \n",
       "\n",
       "                                      gemma2_queries  \\\n",
       "0  [What type of containers are ideal for collect...   \n",
       "1  [What colors are ideal for a minimalist table ...   \n",
       "\n",
       "                                      gemma2_answers  doc_id  \\\n",
       "0  [Containers like baking trays, takeaway food c...       0   \n",
       "1  [Black and white are ideal, with wood providin...       1   \n",
       "\n",
       "                                      llama3_queries  \\\n",
       "0  [What's the best way to sterilize a container ...   \n",
       "1  [What colors make a good minimalist palette?, ...   \n",
       "\n",
       "                                      llama3_answers  \n",
       "0  [You can sterilize a container at home using s...  \n",
       "1  [Select black, white, and wood colors for a cl...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_indices_to_drop(sample_df):\n",
    "    indices_to_drop = []\n",
    "    for index, row in sample_df.iterrows():\n",
    "        queries = row['gemma2_queries']\n",
    "        answers = row['gemma2_answers']\n",
    "        \n",
    "        llama3_answers = row['llama3_answers']\n",
    "        \n",
    "        for j, answer in enumerate(llama3_answers):\n",
    "            # Check if the answer is a dictionary\n",
    "            if isinstance(answer, dict):  # Using isinstance to check the type\n",
    "                #print(f\"Index: {index}\")  # Print the index of the row\n",
    "                indices_to_drop.append(index)\n",
    "                #print(f\"A{j+1}: {answer} (Type: {type(answer).__name__})\")\n",
    "                \n",
    "\n",
    "        for j, answer in enumerate(answers):\n",
    "            # Check if the answer is a dictionary\n",
    "            if isinstance(answer, dict):  # Using isinstance to check the type\n",
    "                #print(f\"Index: {index}\")  # Print the index of the row\n",
    "                indices_to_drop.append(index)\n",
    "                #print(f\"A{j+1}: {answer} (Type: {type(answer).__name__})\")\n",
    "\n",
    "    unique_indices_to_drop = list(set(indices_to_drop))\n",
    "    return unique_indices_to_drop\n",
    "\n",
    "\n",
    "def clear_generated_query_answer_data(file_name):\n",
    "    df= pd.read_parquet(file_name)\n",
    "    \n",
    "    df = df[df['gemma2_queries'].apply(lambda x: len(x) != 0) & df['llama3_queries'].apply(lambda x: len(x) != 0)]\n",
    "    \n",
    "    df['gemma2_answers'] = df['gemma2_answers'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    df['llama3_answers'] = df['llama3_answers'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    unique_indices_to_drop = get_indices_to_drop(df)\n",
    "    df.drop(index=unique_indices_to_drop, inplace=True)\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(df.shape)\n",
    "    df.to_parquet(WIKI_QUERY_ANSWER_GENERATED_FILE)\n",
    "\n",
    "    # Explode the 'LLM_Query' column and\n",
    "    df_exploded_gemma2 = df.explode(['gemma2_queries', 'gemma2_answers'])\n",
    "    df_exploded_gemma2.reset_index(drop=True, inplace=True)\n",
    "    df_exploded_gemma2.drop(columns=['llama3_queries','llama3_answers'], inplace=True)\n",
    "    df_exploded_gemma2.to_parquet(GEMMA2_QA_DATA_FILE)\n",
    "    \n",
    "    \n",
    "    # Explode the 'LLM_Query' column and\n",
    "    df_exploded_llama3 = df.explode(['llama3_queries','llama3_answers'])\n",
    "    df_exploded_llama3.reset_index(drop=True, inplace=True)\n",
    "    df_exploded_llama3.drop(columns=['gemma2_queries', 'gemma2_answers'], inplace=True)\n",
    "    df_exploded_llama3.to_parquet(LLAMA3_QA_DATA_FILE)\n",
    "    \n",
    "    # Display the result\n",
    "    return df, df_exploded_gemma2,df_exploded_llama3\n",
    "\n",
    "raw_qa_df, gemma_qa_df,llama_qa_df = clear_generated_query_answer_data(WIKI_QUERY_ANSWER_RAW_FILE)\n",
    "raw_qa_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f002ef",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge :: Evaluating the quality of generated q & a data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81257a40",
   "metadata": {},
   "source": [
    "#### Defining prompts for Evaluating the Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7bf38e68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T14:58:57.578365Z",
     "start_time": "2024-09-29T14:58:57.567300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define evaluation prompt template\n",
    "Q_A_EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI evaluator tasked with assessing the relevance and effectiveness of user answers generated for a specific query.\n",
    "Your evaluation will help refine the answer generation process.\n",
    "########\n",
    "Instructions:\n",
    "Evaluate the relevance, clarity,overall quality of the provided queries.\n",
    "Score each answer from 1-5 based on the following criteria:\n",
    "- Relevance: Does the answer directly or indirectly related to the query, although not focused?\n",
    "- Clarity: How easy is the answer to understand?\n",
    "- Quality: Does the answer demonstrate user needs?\n",
    "\n",
    "Provide constructive feedback for improvement.\n",
    "########\n",
    "Now, evaluate the user answer based on the following query:\n",
    "{query}\n",
    "answer:\n",
    "{answer}\n",
    "\n",
    "Provide your evaluation in the following format:\n",
    "Score: {{score}} (1-5)\n",
    "Feedback: {{feedback}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f543bef",
   "metadata": {},
   "source": [
    "#### Evaluate the quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "daf660ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T14:58:59.503059Z",
     "start_time": "2024-09-29T14:58:59.478182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define llm model\n",
    "def set_evaluate_model_prompt_chain_qa(evaluate_llm_model):\n",
    "    \n",
    "    model_evaluate_synthetic = Ollama(model=evaluate_llm_model)\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "            template=EVALUATION_PROMPT_TEMPLATE,\n",
    "            input_variables=[\"query\", \"answer\"],\n",
    "        )\n",
    "\n",
    "    evaluation_chain = prompt_template | model_evaluate_synthetic\n",
    "    return evaluation_chain \n",
    "\n",
    "\n",
    "def llm_evaluate_queries_and_answers_for_context(query: str, answer: str,evaluation_chain,evaluate_llm_model):\n",
    "    \"\"\"Evaluate answer using LLaMA model.\"\"\"\n",
    "    try:\n",
    "\n",
    "        llm_response = evaluation_chain.invoke({\"query\": query, \"answer\": answer})\n",
    "        \n",
    "        llm_response = llm_response.replace('**', '').replace('*', '')  # Remove asterisks\n",
    "        llm_response = llm_response.replace(':', ': ')  # Standardize colon spacing\n",
    "        \n",
    "        #print(llm_response)\n",
    "        if(evaluate_llm_model==\"llama3\" or evaluate_llm_model==\"gemma2:2b\"):\n",
    "            FEEDBACK_PATTERN = r'Feedback: (.*)\\n'\n",
    "            feedbacks = re.findall(FEEDBACK_PATTERN, llm_response, re.MULTILINE)\n",
    "            \n",
    "            scores = []\n",
    "            SCORE_PATTERN = r'Score:\\s*(\\d+)'\n",
    "            matches = re.findall(SCORE_PATTERN, llm_response)\n",
    "\n",
    "            if matches:\n",
    "                scores = [int(match) for match in matches]\n",
    "                \n",
    "            return scores, feedbacks\n",
    "        \n",
    "        else:\n",
    "            FEEDBACK_PATTERN = r'Feedback:\\s*(.*?)(?=\\n|$)'\n",
    "            SCORE_PATTERN = r'\\*\\*Score:\\*\\* (\\d+)'\n",
    "\n",
    "            # Extract the scores\n",
    "            scores = [int(score) for score in re.findall(SCORE_PATTERN, llm_response)]\n",
    "            print(scores)\n",
    "\n",
    "             # Extract feedbacks\n",
    "            feedbacks_raw = re.findall(FEEDBACK_PATTERN, llm_response)\n",
    "\n",
    "            # Append feedbacks one by one to the list\n",
    "            feedbacks = []\n",
    "            for feedback in feedbacks_raw:\n",
    "                feedback = feedback.replace('**', '')\n",
    "                feedbacks.append(feedback.strip()) \n",
    "\n",
    "            return scores, feedbacks\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e055a0f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T05:52:45.844701Z",
     "start_time": "2024-09-29T05:52:27.365715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:  Choose tableware with clean lines and minimal patterns. Opt for entirely white ceramic plates and plain white or grey napkins.\n",
      "## Evaluation of User Answer: \n",
      "\n",
      "Score:  4 \n",
      "\n",
      "Feedback:   The answer demonstrates a good understanding of \"minimalist\" aesthetics and offers specific practical recommendations. It suggests using tableware with clean lines and minimal patterns, which aligns directly with the query's focus on minimalist tablescapes. The inclusion of white ceramic plates and plain napkins further reinforces this concept.\n",
      "\n",
      "Here are some suggestions for improvement: \n",
      "\n",
      " Expand on \"minimal patterns\":   The answer could benefit from elaborating on what constitutes \"minimal patterns.\" Providing examples like single dots, subtle geometric designs, or even a single floral accent would enhance clarity and provide more specific guidance. \n",
      " Contextualize choices:  Briefly mentioning the benefits of minimalism (e.g., promoting visual simplicity, fostering a sense of calm) could add depth to the answer.  \n",
      " Offer additional options:  While white is common, suggesting other color palettes commonly used in minimalist tablescapes (e.g., grey tones, black, natural wood finishes) could provide more creative avenues for exploration.  \n",
      "\n",
      "\n",
      "\n",
      "Overall, this answer serves as a good starting point and can be further refined to provide a more comprehensive and practical guide to choosing tableware for a minimalist tablescape. \n",
      "\n",
      "Answer Evaluation results\n",
      "[4]\n",
      "['  The answer demonstrates a good understanding of \"minimalist\" aesthetics and offers specific practical recommendations. It suggests using tableware with clean lines and minimal patterns, which aligns directly with the query\\'s focus on minimalist tablescapes. The inclusion of white ceramic plates and plain napkins further reinforces this concept.']\n"
     ]
    }
   ],
   "source": [
    "### Test evaluation\n",
    "EVALUATION_MODEL = \"gemma2:2b\"\n",
    "evaluation_chain = set_evaluate_model_prompt_chain_qa(EVALUATION_MODEL)\n",
    "query = llama_df_qa.iloc[6]['llama3_queries']\n",
    "#print('Query: ', query)\n",
    "\n",
    "# Generate queries\n",
    "answer  =  llama_df_qa.iloc[6]['llama3_answers']\n",
    "print('Generated Answer: ', answer)\n",
    "\n",
    "# Evaluate queries\n",
    "scores,feedbacks = llm_evaluate_queries_and_answers_for_context(query, answer,evaluation_chain,EVALUATION_MODEL)\n",
    "print('Answer Evaluation results')\n",
    "print(scores)\n",
    "print(feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01779686",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T05:53:06.161516Z",
     "start_time": "2024-09-29T05:52:45.845749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:  Use neutral colors in your furniture and decor, aiming for a clean and cohesive look.\n",
      "Here is my evaluation: \n",
      "\n",
      "Score:  3\n",
      "Feedback:  The answer partially addresses the query about minimalist table design. While it provides some general tips on achieving a clean and cohesive look, it does not directly relate to designing tables. The suggestion to use neutral colors is relevant but vague and could apply to any interior design, not specifically tables.\n",
      "\n",
      "To improve this answer: \n",
      "\n",
      " Provide more specific and concrete suggestions related to table design, such as using simple shapes, reducing ornamentation, or incorporating natural materials.\n",
      " Clarify how the suggested approach can be applied to table design in particular, rather than just general interior design.\n",
      "Answer Evaluation results\n",
      "[3]\n",
      "[' The answer partially addresses the query about minimalist table design. While it provides some general tips on achieving a clean and cohesive look, it does not directly relate to designing tables. The suggestion to use neutral colors is relevant but vague and could apply to any interior design, not specifically tables.']\n"
     ]
    }
   ],
   "source": [
    "### Test evaluation\n",
    "EVALUATION_MODEL = \"llama3\"\n",
    "evaluation_chain = set_evaluate_model_prompt_chain_qa(EVALUATION_MODEL)\n",
    "query = gemma_qa_df.iloc[6]['gemma2_queries']\n",
    "#print('Query: ', query)\n",
    "\n",
    "# Generate queries\n",
    "answer  =  gemma_qa_df.iloc[6]['gemma2_answers']\n",
    "print('Generated Answer: ', answer)\n",
    "\n",
    "# Evaluate queries\n",
    "scores,feedbacks = llm_evaluate_queries_and_answers_for_context(query, answer,evaluation_chain,EVALUATION_MODEL)\n",
    "print('Answer Evaluation results')\n",
    "print(scores)\n",
    "print(feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d8702d47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T05:53:30.613569Z",
     "start_time": "2024-09-29T05:53:30.603313Z"
    }
   },
   "outputs": [],
   "source": [
    "def llm_evaluate_queries_and_answers_for_context_helper(row,evaluation_chain,EVALUATION_MODEL,query_col_name, answer_col_name):\n",
    "    scores,feedbacks = llm_evaluate_queries_and_answers_for_context(row[query_col_name], row[answer_col_name],evaluation_chain,EVALUATION_MODEL)  # Generate queries based on the text\n",
    "    return pd.Series([scores, feedbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "93ae2663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T07:33:32.911687Z",
     "start_time": "2024-09-29T05:54:36.175598Z"
    }
   },
   "outputs": [],
   "source": [
    "EVALUATION_MODEL = \"gemma2:2b\"\n",
    "evaluation_chain = set_evaluate_model_prompt_chain_qa(EVALUATION_MODEL)\n",
    "\n",
    "llama_df_qa[['llama3_queries_gemma_score', 'llama3_queries_gemma_feedback']] = llama_df_qa.apply(\n",
    "    lambda row: llm_evaluate_queries_and_answers_for_context_helper(row, evaluation_chain, EVALUATION_MODEL, \n",
    "                                                        query_col_name='llama3_queries',answer_col_name='llama3_answers'), axis=1)\n",
    "llama_df_qa.to_parquet(GEMMA2_QA_DATA_EVALUATED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b31a14",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-29T14:59:33.172Z"
    }
   },
   "outputs": [],
   "source": [
    "EVALUATION_MODEL = \"llama3\"\n",
    "evaluation_chain = set_evaluate_model_prompt_chain_qa(EVALUATION_MODEL)\n",
    "\n",
    "gemma_qa_df[['gemma2_queries_llama3_score', 'gemma2_queries_llama3_feedback']] = gemma_qa_df.apply(\n",
    "    lambda row: llm_evaluate_queries_and_answers_for_context_helper(row, evaluation_chain, EVALUATION_MODEL, \n",
    "                                                        query_col_name='gemma2_queries', answer_col_name='gemma2_answers'), axis=1)\n",
    "gemma_qa_df.to_parquet(LLAMA3_QA_DATA_EVALUATED_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f67a2d",
   "metadata": {},
   "source": [
    "## Option2: LLM :: To Generate only q from context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131779d3",
   "metadata": {},
   "source": [
    "### Prompt for Generating Synthetic data q from context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9d546a69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T07:34:06.981119Z",
     "start_time": "2024-09-29T07:34:06.969312Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define prompt template for generating questions\n",
    "\n",
    "QUERY_GENERATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI assistant designed to generate realistic user queries related to various topics.\n",
    "Your task is to create diverse, natural-sounding questions that reflect what actual users might ask when seeking \n",
    "information on a specific subject.\n",
    "\n",
    "Generate 5 diverse, natural, short, specific, and concise user queries (not more than 10 words) related to the given topic.\n",
    "Ensure queries vary in complexity, specificity, and format.\n",
    "Mimic real users seeking information on a chatbot platform.\n",
    "Use simple language and maintain a neutral, conversational tone.\n",
    "\n",
    "**Guidelines:**\n",
    "1. Vary complexity, specificity, and format.\n",
    "2. Mimic chatbot users seeking information.\n",
    "3. Use simple language and maintain a neutral tone.\n",
    "4. Avoid ambiguity and ensure specificity.\n",
    "\n",
    "Example Input:\n",
    "Topic: ### Planning a Budget-Friendly Vacation \n",
    "Planning a budget-friendly vacation requires careful research and flexibility.\n",
    "\n",
    "Example Queries:\n",
    "What are the best budget-friendly destinations worldwide?\n",
    "How can I save money on flights and accommodations?\n",
    "What's the cheapest way to travel across Europe?\n",
    "How do I plan a 5-day budget trip from Singapore to Hawaii?\n",
    "What budgeting apps are best for travel expenses?\n",
    "\n",
    "Generate the output in the list of string format\n",
    "{format_instructions}\n",
    "\n",
    "Now, generate user queries based on the following topic:\n",
    "{topic}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48240389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T04:20:15.571429Z",
     "start_time": "2024-09-29T04:20:15.563304Z"
    }
   },
   "source": [
    "### Generating Synthetic data q from context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "df458dcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T07:34:08.459793Z",
     "start_time": "2024-09-29T07:34:08.437881Z"
    }
   },
   "outputs": [],
   "source": [
    "## https://nanonets.com/blog/langchain/\n",
    "def set_generate_model_prompt_chain(GENRATIVE_MODEL):\n",
    "    \n",
    "    class LLMGenerate_OutputFormat(BaseModel):\n",
    "        \"\"\"LLM output format\"\"\"\n",
    "        responses: List[str] = Field(description=\"List of string responses generated by the LLM\",\n",
    "            min_items=1)\n",
    "        \n",
    "    model_generate_synthetic = Ollama(model=GENRATIVE_MODEL)\n",
    "    \n",
    "    output_parser = PydanticOutputParser(pydantic_object=LLMGenerate_OutputFormat)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=QUERY_GENERATION_PROMPT_TEMPLATE,\n",
    "        input_variables=[\"topic\"],\n",
    "        partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    "    )\n",
    "    \n",
    "    chain = prompt | model_generate_synthetic \n",
    "    \n",
    "    return chain \n",
    "    \n",
    "\n",
    "def llm_generate_queries_for_context(topic,chain,GENRATIVE_MODEL):\n",
    "    \"\"\"\n",
    "    Generate queries for a given context topic using LLaMA.\n",
    "\n",
    "    Args:\n",
    "    topic (str): Context topic.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of generated queries.\n",
    "\n",
    "    Raises:\n",
    "    Exception: If LLaMA invocation fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        queries = chain.invoke({\"topic\": topic})\n",
    "        \n",
    "        if(GENRATIVE_MODEL!=\"llama3.2\"):\n",
    "            \n",
    "            llm_output_str = queries.strip()\n",
    "\n",
    "            # Find the JSON part\n",
    "            json_start = llm_output_str.find(\"{\")\n",
    "            json_end = llm_output_str.rfind(\"}\")\n",
    "\n",
    "            # Extract JSON and Parse JSON\n",
    "            json_str = llm_output_str[json_start:json_end+1]\n",
    "            llm_output = json.loads(json_str)\n",
    "\n",
    "            # Extract responses\n",
    "            responses = llm_output[\"responses\"]\n",
    "\n",
    "            return responses\n",
    "        else:\n",
    "            llm_output_str = queries.strip()\n",
    "\n",
    "            # Find the JSON part\n",
    "            json_start = llm_output_str.find(\"{\")\n",
    "            json_end = llm_output_str.rfind(\"}\")\n",
    "\n",
    "            # Extract JSON and Parse JSON\n",
    "            json_str = llm_output_str[json_start:json_end+1]\n",
    "            llm_output = json.loads(json_str)\n",
    "\n",
    "            # Extract responses\n",
    "            responses = llm_output[\"responses\"]\n",
    "\n",
    "            return responses\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c58d9a",
   "metadata": {},
   "source": [
    "### Applying Synthetic data q  Generation from context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b6fb2010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T08:25:04.811739Z",
     "start_time": "2024-09-29T07:34:19.365900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting ',' delimiter: line 1 column 309 (char 308)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Unterminated string starting at: line 1 column 253 (char 252)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "GENRATIVE_MODEL = \"gemma2:2b\"\n",
    "generation_chain = set_generate_model_prompt_chain(GENRATIVE_MODEL)\n",
    "\n",
    "def llm_generate_queries_for_context_helper(row):\n",
    "    queries = llm_generate_queries_for_context(row['text'],generation_chain,GENRATIVE_MODEL)  # Generate queries based on the text\n",
    "    return queries  \n",
    "\n",
    "df = pd.read_parquet(WIKI_SAMPLE_CLEANED_FILE)\n",
    "# Apply the function to each row in the DataFrame and create a new column 'queries'\n",
    "df['gemma_queries_sample'] = df.apply(llm_generate_queries_for_context_helper, axis=1)\n",
    "df.to_parquet(WIKI_QUERY_RAW_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa9feeec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T10:24:48.738189Z",
     "start_time": "2024-09-29T08:25:04.814751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Expecting ',' delimiter: line 4 column 1 (char 77)\n",
      "Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "GENRATIVE_MODEL = \"llama3\"\n",
    "generation_chain = set_generate_model_prompt_chain(GENRATIVE_MODEL)\n",
    "\n",
    "def llm_generate_queries_for_context_helper(row):\n",
    "    queries = llm_generate_queries_for_context(row['text'],generation_chain,GENRATIVE_MODEL)  # Generate queries based on the text\n",
    "    return queries  \n",
    "\n",
    "\n",
    "df=pd.read_parquet(WIKI_QUERY_RAW_FILE)\n",
    "\n",
    "# Apply the function to each row in the DataFrame and create a new column 'queries'\n",
    "df['llama3_queries_sample'] = df.apply(llm_generate_queries_for_context_helper, axis=1)\n",
    "df['doc_id'] = df.index\n",
    "df.to_parquet(WIKI_QUERY_RAW_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d277b4",
   "metadata": {},
   "source": [
    "### Clean the Synthetic data generated q file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "30445d20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T10:51:46.979493Z",
     "start_time": "2024-09-29T10:51:46.886239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>gemma_queries_sample</th>\n",
       "      <th>llama3_queries_sample</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Choose your container. Understand why steriliz...</td>\n",
       "      <td>The ideal container is wide, flat and shallow,...</td>\n",
       "      <td>[What's the best way to sterilize a container ...</td>\n",
       "      <td>[What's the best way to sterilize a container ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avoid bright colors. Choose monochromatic elem...</td>\n",
       "      <td>The ideal palette for a minimalist tablescape ...</td>\n",
       "      <td>[What colors are best for a minimalist tablesc...</td>\n",
       "      <td>[What's the most effective way to balance blac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  Choose your container. Understand why steriliz...   \n",
       "1  Avoid bright colors. Choose monochromatic elem...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The ideal container is wide, flat and shallow,...   \n",
       "1  The ideal palette for a minimalist tablescape ...   \n",
       "\n",
       "                                gemma_queries_sample  \\\n",
       "0  [What's the best way to sterilize a container ...   \n",
       "1  [What colors are best for a minimalist tablesc...   \n",
       "\n",
       "                               llama3_queries_sample  doc_id  \n",
       "0  [What's the best way to sterilize a container ...       0  \n",
       "1  [What's the most effective way to balance blac...       1  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clear_generated_query_data(file_name):\n",
    "    df= pd.read_parquet(file_name)\n",
    "    \n",
    "    df = df[df['gemma_queries_sample'].apply(lambda x: len(x) != 0) & df['llama3_queries_sample'].apply(lambda x: len(x) != 0)]\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(df.shape)\n",
    "    df.to_parquet(WIKI_QUERY_GENERATED_FILE)\n",
    "\n",
    "    # Explode the 'LLM_Query' column and\n",
    "    df_exploded_gemma2 = df.explode(['gemma_queries_sample'])\n",
    "    df_exploded_gemma2.reset_index(drop=True, inplace=True)\n",
    "    df_exploded_gemma2.drop(columns=['llama3_queries_sample'], inplace=True)\n",
    "    df_exploded_gemma2.to_parquet(GEMMA2_QUERY_FILE)\n",
    "\n",
    "    # Explode the 'LLM_Query' column and\n",
    "    df_exploded_llama3 = df.explode(['llama3_queries_sample'])\n",
    "    df_exploded_llama3.reset_index(drop=True, inplace=True)\n",
    "    df_exploded_llama3.drop(columns=['gemma_queries_sample'], inplace=True)\n",
    "    df_exploded_llama3.to_parquet(LLAMA3_QUERY_FILE)\n",
    "    \n",
    "    # Display the result\n",
    "    return df, df_exploded_gemma2,df_exploded_llama3\n",
    "\n",
    "raw_query_df, gemma_query_df,llama_query_df = clear_generated_query_data(WIKI_QUERY_RAW_FILE)\n",
    "raw_query_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7381aa",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge :: Evaluating the quality of generated q data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704d589",
   "metadata": {},
   "source": [
    "#### Defining prompts for Evaluating the Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "840d9d5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T10:53:02.478890Z",
     "start_time": "2024-09-29T10:53:02.471136Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define evaluation prompt template\n",
    "Q_EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI evaluator tasked with assessing the relevance and effectiveness of user queries generated for a specific topic.\n",
    "Your evaluation will help refine the query generation process.\n",
    "########\n",
    "Instructions:\n",
    "Evaluate the relevance, clarity,overall quality of the provided queries.\n",
    "Score each query from 1-5 based on the following criteria:\n",
    "- Relevance: Does the Query directly or indirectly related to the topic, although not focused?\n",
    "- Clarity: How easy is the query to understand?\n",
    "- Quality: Does the Query demonstrate user needs?\n",
    "\n",
    "Provide constructive feedback for improvement.\n",
    "########\n",
    "Now, evaluate the user queries based on the following topic:\n",
    "{topic}\n",
    "Queries:\n",
    "{queries}\n",
    "\n",
    "Provide your evaluation in the following format:\n",
    "\n",
    "Query {{number}}: {{query}}\n",
    "Score: {{score}} (1-5)\n",
    "Feedback: {{feedback}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d5c61",
   "metadata": {},
   "source": [
    "#### Evaluate the quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "96c43075",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T10:53:07.054961Z",
     "start_time": "2024-09-29T10:53:07.034370Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define llm model\n",
    "def set_evaluate_model_prompt_chain(evaluate_llm_model):\n",
    "    \n",
    "    model_evaluate_synthetic = Ollama(model=evaluate_llm_model)\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "            template=Q_EVALUATION_PROMPT_TEMPLATE,\n",
    "            input_variables=[\"topic\", \"queries\"],\n",
    "        )\n",
    "\n",
    "    evaluation_chain = prompt_template | model_evaluate_synthetic\n",
    "    return evaluation_chain \n",
    "\n",
    "\n",
    "def llm_evaluate_queries_for_context(topic: str, queries: List[str],evaluation_chain,evaluate_llm_model):\n",
    "    \"\"\"Evaluate queries using LLaMA model.\"\"\"\n",
    "    try:\n",
    "\n",
    "        llm_response = evaluation_chain.invoke({\"topic\": topic, \"queries\": queries})\n",
    "        \n",
    "        llm_response = llm_response.replace('**', '').replace('*', '')  # Remove asterisks\n",
    "        llm_response = llm_response.replace(':', ': ')  # Standardize colon spacing\n",
    "        \n",
    "        if(evaluate_llm_model==\"llama3\" or evaluate_llm_model==\"gemma2:2b\"):\n",
    "            FEEDBACK_PATTERN = r'Feedback: (.*)\\n'\n",
    "            feedbacks = re.findall(FEEDBACK_PATTERN, llm_response, re.MULTILINE)\n",
    "            \n",
    "            scores = []\n",
    "            SCORE_PATTERN = r'Score:\\s*(\\d+)'\n",
    "            matches = re.findall(SCORE_PATTERN, llm_response)\n",
    "\n",
    "            if matches:\n",
    "                scores = [int(match) for match in matches]\n",
    "                \n",
    "            return scores, feedbacks\n",
    "        \n",
    "        else:\n",
    "            FEEDBACK_PATTERN = r'Feedback:\\s*(.*?)(?=\\n|$)'\n",
    "            SCORE_PATTERN = r'\\*\\*Score:\\*\\* (\\d+)'\n",
    "\n",
    "            # Extract the scores\n",
    "            scores = [int(score) for score in re.findall(SCORE_PATTERN, llm_response)]\n",
    "            print(scores)\n",
    "\n",
    "             # Extract feedbacks\n",
    "            feedbacks_raw = re.findall(FEEDBACK_PATTERN, llm_response)\n",
    "\n",
    "            # Append feedbacks one by one to the list\n",
    "            feedbacks = []\n",
    "            for feedback in feedbacks_raw:\n",
    "                feedback = feedback.replace('**', '')\n",
    "                feedbacks.append(feedback.strip()) \n",
    "\n",
    "            return scores, feedbacks\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0f44d0eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T10:53:08.125871Z",
     "start_time": "2024-09-29T10:53:08.121308Z"
    }
   },
   "outputs": [],
   "source": [
    "def llm_evaluate_queries_for_context_helper(row,evaluation_chain,EVALUATION_MODEL,query_col_name):\n",
    "    scores,feedbacks = llm_evaluate_queries_for_context(row['text'], row[query_col_name],evaluation_chain,EVALUATION_MODEL)  # Generate queries based on the text\n",
    "    return pd.Series([scores, feedbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "502bee2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T13:26:01.239115Z",
     "start_time": "2024-09-29T10:53:09.554462Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EVALUATION_MODEL = \"llama3\"\n",
    "evaluation_chain = set_evaluate_model_prompt_chain(EVALUATION_MODEL)\n",
    "\n",
    "raw_query_df[['gemma_queries_llama3_score', 'gemma_queries_llama3_feedback']] = raw_query_df.apply(\n",
    "    lambda row: llm_evaluate_queries_for_context_helper(row, evaluation_chain, EVALUATION_MODEL, \n",
    "                                                        query_col_name='gemma_queries_sample'), axis=1)\n",
    "\n",
    "raw_query_df.to_parquet(WIKI_QUERY_EVALUATED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7bc71359",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T14:57:36.022520Z",
     "start_time": "2024-09-29T13:37:24.948111Z"
    }
   },
   "outputs": [],
   "source": [
    "EVALUATION_MODEL = \"gemma2:2b\"\n",
    "evaluation_chain = set_evaluate_model_prompt_chain(EVALUATION_MODEL)\n",
    "raw_query_df = pd.read_parquet(WIKI_QUERY_EVALUATED_FILE)\n",
    "\n",
    "raw_query_df[['llama3_queries_gemma_score', 'llama3_queries_gemma_feedback']] = raw_query_df.apply(\n",
    "    lambda row: llm_evaluate_queries_for_context_helper(row, evaluation_chain, EVALUATION_MODEL, \n",
    "                                                        query_col_name='llama3_queries_sample'), axis=1)\n",
    "\n",
    "raw_query_df.to_parquet(WIKI_QUERY_EVALUATED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c8c5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
